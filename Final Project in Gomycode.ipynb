{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e644e3a0",
   "metadata": {},
   "source": [
    "**IMPORT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36862f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import linear algebra and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import libraries for machine learning models and evaluation\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For scaling numerical data and encoding categorical data\n",
    "\n",
    "\n",
    "#import standard visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import the label Encoder library \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87694aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe43a7-c704-447d-a96e-682a3d5c11cc",
   "metadata": {},
   "source": [
    "**Import the CSV File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f085d5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb78694-3284-434b-8434-20a1cdf27208",
   "metadata": {},
   "source": [
    "**Preprocessing & Cleaning of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19552858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"object\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbe535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"number\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Unnecassary Columns\n",
    "df = df.drop(['Row ID', 'Order ID', 'Customer ID', 'Customer Name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd15bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['State'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Country', 'Product Name', 'Postal Code', 'Product ID', 'City'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the duplicated rows\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing With the missing value in postal code column\n",
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc88cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Order Date']= pd.to_datetime(df['Order Date'],dayfirst=True)\n",
    "df['Ship Date']= pd.to_datetime(df['Ship Date'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long time to deliver products\n",
    "df[\"Delivery Time\"] = (df[\"Ship Date\"] - df[\"Order Date\"]).dt.days\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f690520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Order Date', 'Ship Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2117bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4097cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ship Mode'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19e70c-ebc2-4b21-a004-4c6792633d28",
   "metadata": {},
   "source": [
    "**Data Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fe163",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include='object').columns\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1975c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 50))\n",
    "for i in range(0, len(categorical_features)):\n",
    "    plt.subplot(11, 2, i+1)\n",
    "    sns.countplot(x = df[categorical_features[i]], palette = 'viridis')\n",
    "    plt.title(categorical_features[i], fontsize = 30)\n",
    "    plt.xlabel(' ')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include='number').columns\n",
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 25))\n",
    "for i in range(0, len(numerical_features)):\n",
    "    plt.subplot(10, 4, i+1)\n",
    "    sns.boxplot(x = df[numerical_features[i]], palette = 'viridis')\n",
    "    plt.title(numerical_features[i], fontsize = 30)\n",
    "    plt.xlabel(' ')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa7778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Correlation matrix of numerical features\n",
    "plt.figure(figsize=(24, 10))\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405cd6b-a53f-4f17-b87b-551db6650e43",
   "metadata": {},
   "source": [
    "**Encoding The Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7993e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_mode_encoding = { 'Same Day': 1, 'First Class': 2, 'Second Class': 3, 'Standard Class': 4}\n",
    "df['Ship Mode Encoded'] = df['Ship Mode'].map(ship_mode_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "columns_encode = ['Category', 'Segment', 'Region', 'Sub-Category']\n",
    "df_encoded = encoder.fit_transform(df[columns_encode])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_column = encoder.get_feature_names_out(columns_encode)\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=df_encoded_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38845a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40be358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Category', 'Region', 'Ship Mode', 'Segment', 'Sub-Category'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac38f5-7b34-4491-8e34-d2c05e271d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3655538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Superstore_sales_prediction_cleaned', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3b6dc-94f7-4cbf-8116-6317799d193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Delivery Time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e7471-39ca-4355-928a-dcf4eb890954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ship Mode Encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fe96c-8492-493d-973d-eacca2094306",
   "metadata": {},
   "source": [
    "**Importing Necessary Libaries Used in Training our Model Using Deep Learning Language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # TensorFlow library for building neural networks\n",
    "from tensorflow.keras.models import Sequential  # Sequential model for building a feedforward neural network\n",
    "from tensorflow.keras.layers import Dense  # Dense layer for fully connected layers\n",
    "from sklearn.model_selection import train_test_split  # Function to split data into training and test sets\n",
    "from sklearn.preprocessing import StandardScaler  # Standardization of features (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Sales'])  # Drop the target column to get features\n",
    "y = df['Sales']  # Select the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc911dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (normalize features) so they have a mean of 0 and variance of 1.\n",
    "# This is important for better convergence in neural networks.\n",
    "scaler = StandardScaler()  # Create an instance of the StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data and apply the transformation\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply the same transformation to test data (without fitting again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965774ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  # Initialize a Sequential model (layers will be added one by one)\n",
    "\n",
    "# Input layer (17 features), followed by the first hidden layer with 32 neurons and ReLU activation\n",
    "model.add(Dense(32, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "# Second hidden layer with 32 neurons and ReLU activation\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Output layer for regression (predicting house prices), single neuron as output since it's a regression problem\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f0994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is your DataFrame containing features and 'Sales' as the target column\n",
    "X = df.drop(\"Sales\", axis=1)\n",
    "y = df[\"Sales\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Sequential model for regression\n",
    "model = Sequential()\n",
    "\n",
    "# Use Input layer to define input shape\n",
    "model.add(Input(shape=(len(X.columns),)))  # Define input shape explicitly\n",
    "\n",
    "# Add Dense layers with adjustments\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())  # Optional: add batch normalization for stable learning\n",
    "model.add(Dropout(0.2))  # Lower dropout rate\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer for regression (no activation or linear activation)\n",
    "model.add(Dense(1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model with regression-appropriate loss and metrics\n",
    "model.compile(optimizer=Adam(learning_rate=0.0015), \n",
    "              loss='mse',  # Mean Squared Error for regression\n",
    "              metrics=['mae'])  # Mean Absolute Error as a metric\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model (adjust epochs, batch_size, etc. as needed)\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_test_scaled, y_test), \n",
    "                    epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22933a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
    "                               patience=5,          # Stop after 5 epochs with no improvement\n",
    "                               restore_best_weights=True)  # Restore the best model weights\n",
    "\n",
    "# Compile the model for regression (use MSE or MAE for loss)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Fit the model with EarlyStopping\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_split=0.15, \n",
    "                    verbose=1, \n",
    "                    callbacks=[early_stopping])  # Add the EarlyStopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8dd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data using Mean Absolute Error (MAE)\n",
    "mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Mean Absolute Error on Test Set: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcf272-a1b8-4013-bb76-454279e3273b",
   "metadata": {},
   "source": [
    "**Deployment of My Model Using Streamlit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998920d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project.py\", \"w\") as file:\n",
    "    file.write('''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"OneDrive/Documents/CASMIR'S DOCUMENT/Superstore_sales_prediction_cleaned\")\n",
    "\n",
    "# Assuming 'Diagnosis' column contains the labels for whether a person has breast cancer or not\n",
    "X = df.drop(columns=['Sales'])  # Features\n",
    "y = df['Sales']  # Target labels\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Sequential model for regression\n",
    "model = Sequential()\n",
    "\n",
    "# Use Input layer to define input shape\n",
    "model.add(Dense(16, activation='relu', input_shape=(len(X.columns),)))  # Define input shape explicitly\n",
    "\n",
    "# Add Dense layers with adjustments\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())  # Optional: add batch normalization for stable learning\n",
    "model.add(Dropout(0.2))  # Lower dropout rate\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer for regression (no activation or linear activation)\n",
    "model.add(Dense(1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model with regression-appropriate loss and metrics\n",
    "model.compile(optimizer=Adam(learning_rate=0.0015), \n",
    "              loss='mse',  # Mean Squared Error for regression\n",
    "              metrics=['mae'])  # Mean Absolute Error as a metric\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
    "                               patience=5,          # Stop after 5 epochs with no improvement\n",
    "                               restore_best_weights=True)  # Restore the best model weights\n",
    "\n",
    "# Compile the model for regression (use MSE or MAE for loss)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Fit the model with EarlyStopping\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_split=0.15, \n",
    "                    verbose=1, \n",
    "                    callbacks=[early_stopping])  # Add the EarlyStopping callback\n",
    "\n",
    "# Evaluate the model's performance on the test data using Mean Absolute Error (MAE)\n",
    "mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Mean Absolute Error on Test Set: {mae}')\n",
    "\n",
    "# Define the predict_sales function\n",
    "def predict_sales(input_data):\n",
    "    return model.predict(input_data)\n",
    "              \n",
    "# Streamlit App\n",
    "logo_path = r\"OneDrive/Documents/CASMIR'S DOCUMENT/My Comany.jpg\"\n",
    "st.image(logo_path, use_column_width='auto')\n",
    "\n",
    "# Streamlit app title\n",
    "st.title(\"Eso's & Grandsons Superstore Prediction\")\n",
    "\n",
    "# App description\n",
    "st.write(\"\"\"\n",
    "Predict future sales based on input features like Category, Sub-Category, Region, Segment, Shipmode, Delivery time\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar for user inputs\n",
    "st.header('Input Parameters')\n",
    "\n",
    "def user_input_features():\n",
    "    category = st.selectbox(\"Category\", [\"Furniture\", \"Office Supplies\", \"Technology\"])\n",
    "    region = st.selectbox(\"Region\", [\"East\", \"West\", \"Central\", \"South\"])\n",
    "    sub_category = st.selectbox(\"Sub-Category\", [\"Accessories\", \"Appliances\", \"Art\", \"Blinders\", \"Bookcases\", \"Chairs\", \"Copiers\", \"Envelopes\", \"Fasteners\", \"Furnishings\", \"Labels\", \"Machines\", \"Paper\", \"Phones\", \"Storage\", \"Supplies\", \"Tables\"])\n",
    "    segment = st.selectbox(\"Segment\", [\"Consumer\", \"Corporate\", \"Home Office\"])\n",
    "    ship_mode = st.selectbox(\"Ship Mode Encoded\", [\"Same Day\", \"First Class\", \"Second Class\", \"Standard Class\"])\n",
    "    delivery_time = st.sidebar.number_input(\"Delivery Time (in days)\", min_value=0, max_value=8, value=8)\n",
    "\n",
    "    data = {\n",
    "        'Category': category,\n",
    "        'Region': region,\n",
    "        'Ship Mode Encoded': ship_mode,\n",
    "        'Segment': segment,\n",
    "        'Sub-Category': sub_category,\n",
    "        'Delivery Time': delivery_time\n",
    "    }\n",
    "\n",
    "     # Convert the input data into a DataFrame\n",
    "    features = pd.DataFrame(data, index=[0])\n",
    "\n",
    "    # Convert categorical columns using one-hot encoding\n",
    "    features = pd.get_dummies(features, columns=['Ship Mode Encoded', 'Category', 'Region', 'Sub-Category', 'Segment'])\n",
    "\n",
    "    \n",
    "    # Align the features with the training data (adding missing columns if necessary)\n",
    "    features = features.reindex(columns=X_train.columns, fill_value=0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "input_df = user_input_features()\n",
    "\n",
    "if st.button('Submit'):\n",
    "    # Scale the user input\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    st.write(input_df)\n",
    "\n",
    "    # Display the prediction results\n",
    "    st.subheader('Input Parameters')\n",
    "\n",
    "# Make prediction\n",
    "if st.button(\"Predict Sales\"):\n",
    "    # Scale the input and make a prediction\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    prediction = predict_sales(input_scaled)\n",
    "    st.subheader(\"Predicted Sales\")\n",
    "    st.write(f\"${prediction[0][0]:.2f}\")\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07c8ea-1fcd-4160-af5e-cfdd5bb55bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
